= Xaraya

The current (at the time of writing) ACCU website is based on a long-dead PHP5 framework,
https://github.com/xaraya[Xaraya].
This manages to be even more dead than AIX, and that takes some doing.

Xaraya stores all its content in a MySQL database.
As befits a flexible web framework, the schema for this database is somewhat impenetrable,
and it's not a straightforward matter to extract the existing content.

Various important and less important subsystems are handled in custom Xaraya modules.

* The membership system. All aspects of the ACCU membership system are handled
  here, including payment processing, renewal reminders etc.
* Book reviews. A database and simple web frontend handle book reviews.
* A long dead blogging system.
* TODO: There are others - fill them in.

== Existing content in Xaraya

Content is stored in the Xaraya MySQL database in raw HTML using Ye Olde Latin1
encoding.

We could be content to just extract the HTML and convert it to UTF-8,
However, this means trying to run two parallel sets of CSS, one to style
the Xaraya HTML, and one to style the site and AsciiDoc output.
This doesn't seem a good idea to me.
If possible, we should attempt to convert existing content to AsciiDoc.

Extracting the data from the database is unexpectedly gnarly, as each
different type of content has different attributes in the database,
and so requires slightly different handling to get the meat.

== New content

New journal content is delivered in simple XML.
This appears to be XML exported from (at a guess) FrameMaker.
At present, AIUI, Bob edits this to HTML and feeds it into Xaraya.
There's a Xaraya page for doing this.
It's basically enter article details and then cut/paste your HTML here.
Obviously it would be a Jolly Good Thing if this XML could also be converted into
AsciiDoc in as painless a manner as possible.

Ove the years, the style of HTML has varied a bit.
Earlier articles can be a bit random, but more recent ones seem to have fairly
settled set of conventions.

== Existing content not in Xaraya

This is one important bit of content that's not in Xaraya.
Or at least not all in Xaraya.
Xaraya keeps its article ID, the article title, author, journal, month, year, summary
and categories.
Other data is stored in external `.bib` files.
There is one `.bib` file per journal.
All the journal indexes are done by a system (originally due to Martin) based on reading
this `.bib` file.

Here's a sample `.bib` entry from the manual file:
----
@Article{
    Title=Afterwood
    Note=Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.
    Author=Oldwood, Chris
    Journal=Overload
    Number=143
    Volume=26
    Pages=24
    Year=2018
    Month=February
    ISSN=1354--3172
    URL=/index.php/journals/2461
    PDF=/var/uploads/journals/Overload143.pdf#page=26
    }
----
Note that the content of the Overload and CVu `.bib` files differs slightly.
Here's a CVu sample:
----
@Article{
    Title=Write Less Code!
    Note=Pete Goodliffe helps us avoid unnecessary lines of code.
    Author=Goodliffe, Pete
    Journal=CVu
    Number=3
    Volume=30
    Pages=3-6
    Year=2018
    Month=July
    ISSN=1354-3164
    URL=/index.php/journals/2524
    PDF=/var/uploads/journals/CVu30-3-dNn1.pdf#Page=5
    }
----

This file is processed by a custom tool (Bob's got a Visual Studio C++ project) that spits out
HTML indexes.
This is a problem for me, as:
[loweralpha]
. I don't want to be building an executable as part of a final build chain.
. I especially don't want that executable to be nailed to Windows.

These indexes are all very nice, but there's a problem, in that if we consider the article
metadata as a whole, some of it (e.g. the Xaraya ID used in the current URLs, the author
name as presented in the article, the article title ditto), is in the Xaraya data and some of it
is in the `.bib`.


== Extracting content from Xaraya

To this end, I have wrangled some Python3 tooling.

=== `accu-dump-xar`

This connects to MySQL and dumps out all articles of a particular
type to individual JSON files.
In the process it transliterates content from Latin1 to UTF-8.

Here's what it produces for a sample article (most of the body omitted):
----
{
    "author": "Chris Oldwood",
    "author-email": "",
    "author2": "",
    "author2-email": "",
    "body": "<p>The interviewer slid a pencil ..."
    "category-id": [
        "o143",
        "Process"
    ],
    "category-name": [
        "Overload Journal #143 - February 2018",
        "Process Topics"
    ],
    "date": "2018-02-01T16:13:46",
    "id": 2461,
    "keywords": "",
    "summary": "Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.",
    "title": "Afterwood"
}
----

This can dump out journal pages, conference and general pages, and book reviews.

We end up with a whole pile of JSON files, but at least we can now see what we're dealing with.

==== Example

In this example, we log onto the current ACCU host in one window and use SSH port
forwarding to provide access to the ACCU host MySQL port on our local system.

----
$ ssh -L 13306:localhost:3306 accu@dennis.accu.org
----

With that connection established, we can then run `accu-dump-xar` in another terminal.
In this example, we're dumping all journal articles under directory `articles`.

----
$ ./accu-dump-xar --host localhost --port 13306 -o articles -p password
----

Looking in `articles/journals` you will find it contains a set of JSON files.
Each file contains the Xaraya data for a single article.

To do this, you will need SSH access to `dennis.accu.org` and the Xaraya MySQL password.

=== `accu-json-bib`

Given a set of JSON files, this spits out a `.bib` file containing the metadata contained within
the JSON.
In other words, the metadata contained within Xaraya.
Some is a little processed.
For example, the tool attempts to convert author names to _lastname, firstname_.

Here's a sample entry from the output:
----
@Article{
  Id=2461
  Title=Afterwood
  Author=Oldwood, Chris
  Note=Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.
  Journal=Overload
  Month=February
  Year=2018
  CategoryID=Process
  CategoryName=Process Topics
}
----
And a sample entry for a CVu article:
----
@Article{
  Id=2524
  Title=Write Less Code!
  Author=Goodliffe, Pete
  Note=Pete Goodliffe helps us avoid unnecessary lines of code.
  Journal=CVu
  Month=July
  Year=2018
  CategoryID=Design
  CategoryName=Design of applications and programs
}
----
Attempts are made to tidy some of the above fields.
We have an insufficiently clever go at turning _firstname lastname_ in the bib-friendly
_lastname, firstname_.
We also look out for Notes (summaries) that have HTML.
These are ignored in this tool; when the article is converted to AsciiDoc, the summary
is put at the start of the body.

==== Example

Given a directory `articles/journals` containing JSON article files, as downloaded
in the previous example, the following commands dump out `.bib` files with the JSON
metadata from the JSON files for each journal.

----
$ ./accu-json-bib -j Overload articles/journals/*.json > overload_xaraya.bib
$ ./accu-json-bib -j CVu articles/journals/*.json > cvu_xaraya.bib
----

=== `accu-bib-merge`

Given one of the manually curated `.bib` files and a `.bib` file that's the output from
`accu-json-bib`, this attempts to produce a single `.bib` file containing all the metadata
from both the manual file and the Xaraya metadata.
Some of the manual `.bib` entries contain the Xaraya article ID, in which case entries
are matched on that.
Otherwise we restort to trying to match on journal, year, month and title.

==== Example

We start with the handcrafted `overload_bib.txt` and `cvu_bib.txt` as used
currently.

As noted earlier, `accu-dump-xar` converts the data it downloads from Latin1 to UTF-8.
We need to perform the same conversion on these files too.

----
$ iconv -f ISO8859-1 -t UTF8 overload_bib.txt > overload_txt.bib
$ iconv -f ISO8859-1 -t UTF8 cvu_bib.txt > cvu_txt.bib
----

With character sets aligned, we can now proceed to merge the two `.bib` files to obtain
a single consolidated `.bib` for each journal.

----
$ ./accu-bib-merge overload_txt.bib overload_xaraya.bib > overload.bib
$ ./accu-bib-merge cvu_txt.bib cvu_xaraya.bib > cvu.bib
----

=== `accu-bib`

As indicated elsewhere, I am hoping that we can generate all required indexes using the
Hugo taxonomies system.
In the event that we can't, I have scratched an itch for a cross-platform replacement
for Bob's tool.
`accu-bib` reads `.bib` files and emits output based on an input template.
I hope we won't end up using it.

=== `accu-json-hugo`

Given a set of JSON files and a `.bib`, this emits one file per JSON
file containing the input article either as HTML (prettified, and with
image locations adjusted) or translated to AsciiDoc. In both cases all metadata
is encoded as Hugo frontmatter.  Output looks like this for HTML:

----
---
title: Afterwood
author: Chris Oldwood
date: 2018-02-01T16:13:46
draft: false
type: journal
journals:
- Overload
month: February
year: 2018
aliases:
- /xaraya/journals/2461.html
categories:
- Process Topics
description: Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.
number: 143
volume: 26
pages: 24
contributors: ['Oldwood, Chris']
---
<div class="article-content">
<div class="article-summary">Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.</div>

<p>
 The interviewer slid a pencil and a sheet of A4 paper over in my direction and said “I’d like you to write a little bit of code to print a calendar.” I thought to myself “that doesn’t seem too hard”.
</p>

<p>
 This wasn’t the very start of the interview, there were a few pleasantries exchanged before cutting to the chase. In that opening introduction, I quickly learned that my interviewer did not work in my department or even my programming language of choice. His personal preference was for Java but given that this was a brand new team with only a project manager who hadn’t written a line of code in years, he was drafted in from another team to fill the void. I let out a disguised sigh of relief as I realised I wasn’t going to get quizzed about obscure C++ trivia.
</p>
----

The HTML output adds the following `div` enclosures:

* `article-content`. Encloses the entire article text.
* `article-summary`. Encloses the article summary, positioned at the top of the output.
* `article-bio`. Encloses any author bio indicated by `<p class="bio">`. If present,
  this is moved to the end of the article.

Here's sample AsciiDoc output:

----
---
title: Afterwood
author: Chris Oldwood
date: 2018-02-01T16:13:46
draft: false
journals:
- Overload
month: February
year: 2018
aliases:
- /xaraya/journals/2461.html
categories:
- Process Topics
description: Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.
number: 143
volume: 26
pages: 24
contributors: ['Oldwood, Chris']
---
= Afterwood
:author: Chris Oldwood
:figure-caption!:
:imagesdir: ..


[.lead]
Can you code on paper in an interview? Chris Oldwood recounts his attempts to write a calendar.

The interviewer slid a pencil and a sheet of A4 paper over in my direction and said “I’d like you to write a little bit of code to print a calendar.” I thought to myself “that doesn’t seem too hard”.

This wasn’t the very start of the interview, there were a few pleasantries exchanged before cutting to the chase. In that opening introduction, I quickly learned that my interviewer did not work in my department or even my programming language of choice. His personal preference was for Java but given that this was a brand new team with only a project manager who hadn’t written a line of code in years, he was drafted in from another team to fill the void. I let out a disguised sigh of relief as I realised I wasn’t going to get quizzed about obscure {cpp} trivia.
...
----

Output files are placed in a path as outlined in <<Layout.adoc#_large_binaries_directory_structure, the Layout description>>.

Images are not stored in Xaraya, Instead they're in a directory hierarchy handled by
Apache.
If converting to AsciiDoc, `accu-json-hugo` also emits to standard output a
list of `cp` command lines.
These attempt to copy images from a copy of the website images directory hierarchy
to the place in which they should reside in the new layout.
Links in the generated HTML or AsciiDoc are set to the location in the new layout.

==== Example

Given downloaded JSON files and merged `.bib` files from the previous examples, let's
now generate AsciiDoc files for all journal articles.

----
$ ./accu-json-hugo -j Overload --bib overload.bib --format adoc --include-bio articles/journals/*.json > overload_images.sh
Conversion error Unknown Tag typename in articles/journals/00215.json
Conversion error Unknown Tag typename in articles/journals/00232.json
Conversion error Unknown Tag class in articles/journals/00296.json
...
$ ./accu-json-adoc -j CVu --bib cvu.bib --format adoc --include-bio articles/journals/*.json > cvu_images.sh
Conversion error Unknown Tag ctype.h in articles/journals/01646.json
Conversion error Unknown Tag codelisting in articles/journals/02101.json
Conversion error Unknown Tag codelisting in articles/journals/02165.json
...
----

As you can see, not all the input files can be processed. This is usually due to an unrecognised
HTML tag, but other problems included tables nested beyond 2 levels, images with no `src`
tag and list items without an enclosing list. In each case, the conversion writes a file
e.g. `00215.json.err.html` containing the offending HTML, with a comment at the top
with the article details.

The output files are scripts to move images from their position in the current website
to the correct location in the new article tree. For example:

----
$ head overload_images.sh
cp "./content/images/journals/ol77/ol77_AOP_Figure1_300dpi.png" journal/overload/2007/feb/programming___abstraction_by_design_0.png
cp "./content/images/journals/ol77/ol77_AOP_Figure2_300dpi.png" journal/overload/2007/feb/programming___abstraction_by_design_1.png
cp "./content/images/journals/ol77/ol77_AOP_Figure3_300dpi.png" journal/overload/2007/feb/programming___abstraction_by_design_2.png
cp "./content/images/journals/ol78/Figure1.png" journal/overload/2007/apr/software_product_line_engineering_with_feature_models_0.png
cp "./content/images/journals/ol78/Figure2v2.png" journal/overload/2007/apr/software_product_line_engineering_with_feature_models_1.png
cp "./content/images/journals/ol78/Figure3v2.png" journal/overload/2007/apr/software_product_line_engineering_with_feature_models_2.png
cp "./content/images/journals/ol78/Figure4v2.png" journal/overload/2007/apr/software_product_line_engineering_with_feature_models_3.png
----

If you have access to the ACCU host, you can generate a suitable tarball of images thus:

----
accu@dennis:~/public_html$ tar cvzf images.tar.gz content/images/journals/*/
----

When running the scripts that move images to the correct destination, you will find that
not all the images are found. This seems to be image sources in the HTML that do not
correspond to the actual image location.

=== `accu-xml-tool`

A stand-alone tool for translating either article XML or article HTML to either AsciiDoc or HTML.
