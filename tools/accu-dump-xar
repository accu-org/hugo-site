#!/usr/bin/python3
#
# accu-dump-xar
#
# Dump Xaraya journal files to individal JSON files.

import argparse
import datetime
import json
import pathlib
import sys

import pymysql

def toutf8(s):
    return s.encode('latin1').decode('utf-8')

def dump_articles(db, outputdir, pubtype, pubtypeid):
    propids = { 96: "keywords", 97: "author", 98: "author-email",
                99: "author2", 100: "author2-email" }

    cursor = db.cursor()
    article_sql = """\
select xar_aid, xar_title, xar_summary, xar_body, xar_pubdate
from xar_articles
where xar_pubtypeid={pubtypeid}""".format(pubtypeid=pubtypeid)
    try:
        cursor.execute(article_sql)
        for row in cursor.fetchall():
            article = {
                "id": row[0],
                "title": toutf8(row[1]),
                "summary": toutf8(row[2]),
                "body": toutf8(row[3]),
                "date": datetime.datetime.fromtimestamp(row[4]).isoformat()
            }
            article_id = row[0]
            cursor2 = db.cursor()
            dyndata_sql = """\
select xar_dd_propid, xar_dd_value
from xar_dynamic_data
where xar_dd_itemid={}""".format(article_id)
            cursor2.execute(dyndata_sql)
            for row2 in cursor2.fetchall():
                if row2[0] in propids:
                    article[propids[row2[0]]] = row2[1]
            cat_sql = """\
select xar_name, xar_description from xar_categories join xar_categories_linkage on xar_categories_linkage.xar_cid = xar_categories.xar_cid where xar_categories_linkage.xar_iid = {}""".format(article_id)
            cursor2.execute(cat_sql)
            article["category-id"] = []
            article["category-name"] = []
            for row2 in cursor2.fetchall():
                article["category-id"].append(row2[0])
                article["category-name"].append(row2[1])

            outfile = pathlib.Path(outputdir, pubtype, "{:05}.json".format(article_id))
            outfile.parent.mkdir(parents=True, exist_ok=True)
            with outfile.open('w') as f:
                json.dump(article, f, ensure_ascii=False, sort_keys=True, indent=4)
            cursor2.close()
    except Exception as err:
        print("No articles read: {}.".format(err), file=sys.stderr)
        sys.exit(1)

def dump_bookreviews(db, outputdir):
    cursor = db.cursor()
    article_sql = """\
select xar_rid, xar_title, xar_author, xar_isbn, xar_publisher, xar_pages, xar_price, xar_recommend, xar_rectext, xar_reviewer, xar_cvu, xar_subject, xar_review, xar_created, xar_modified
from xar_bookreviews"""
    try:
        cursor.execute(article_sql)
        for row in cursor.fetchall():
            review = {
                "id": row[0],
                "title": toutf8(row[1]),
                "author": toutf8(row[2]),
                "isbn": toutf8(row[3]),
                "publisher": toutf8(row[4]),
                "pages": toutf8(row[5]),
                "price": toutf8(row[6]),
                "rating": toutf8(row[7]),
                "summary": toutf8(row[8]),
                "reviewer": toutf8(row[9]),
                "cvu": toutf8(row[10]),
                "subject": toutf8(row[11]),
                "review": toutf8(row[12]),
                "created": row[13].isoformat(),
                "modified": row[14].isoformat()
            }
            review_id = row[0]
            outfile = pathlib.Path(outputdir, 'bookreviews', "{:05}.json".format(review_id))
            outfile.parent.mkdir(parents=True, exist_ok=True)
            with outfile.open('w') as f:
                json.dump(review, f, ensure_ascii=False, sort_keys=True, indent=4)
    except Exception as err:
        print("No book reviews read: {}.".format(err), file=sys.stderr)
        sys.exit(1)

def dump_pages(db, outputdir, pagetype, pagetypeid):
    propids = { 26: "body", 27: "page-title", 30: "menu-title",
                28: "page-title", 29: "body", 31: "menu-title", 46: "block" }
    cursor = db.cursor()
    page_sql = """\
select xar_pid, xar_name, xar_desc
from xar_xarpages_pages
where xar_status = 'ACTIVE' and xar_itemtype={pagetypeid}""".format(pagetypeid=pagetypeid)
    try:
        cursor.execute(page_sql)
        for row in cursor.fetchall():
            page = {
                "id": row[0],
                "name": toutf8(row[1]),
                "description": toutf8(row[2])
            }
            page_id = row[0]
            cursor2 = db.cursor()
            dyndata_sql = """\
select xar_dd_propid, xar_dd_value
from xar_dynamic_data
where xar_dd_itemid={}""".format(page_id)
            cursor2.execute(dyndata_sql)
            for row2 in cursor2.fetchall():
                if row2[0] in propids:
                    page[propids[row2[0]]] = row2[1]

            outfile = pathlib.Path(outputdir, pagetype, "{:05}.json".format(page_id))
            outfile.parent.mkdir(parents=True, exist_ok=True)
            with outfile.open('w') as f:
                json.dump(page, f, ensure_ascii=False, sort_keys=True, indent=4)
            cursor2.close()
    except Exception as err:
        print("No articles read: {}.".format(err), file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='dump Xaraya articles to JSON')
    parser.add_argument('--pubtype', dest='pubtype', action='store',
                        choices=['news', 'docs', 'weblinks',
                                 'pdf', 'epub',
                                 'blogs', 'journals',
                                 'accupages', 'conferencepages',
                                 'bookreviews'], default='journals',
                        help='type of publication', metavar='PUBTYPE')
    parser.add_argument('--host', dest='host',
                        action='store', default='localhost',
                        help='database host', metavar='HOSTNAME')
    parser.add_argument('--port', dest='port',
                        action='store', type=int, default='3306',
                        help='database port', metavar='PORT')
    parser.add_argument('-o', '--output-dir', dest='outputdir',
                        action='store', default='.',
                        help='directory for output files', metavar='DIR')
    parser.add_argument('-p', '--password', dest='password',
                        action='store', required=True,
                        help='database password', metavar='PASSWORD')
    args = parser.parse_args()

    pubtypes = { "news": 1, "docs": 2, "weblinks": 6,
                 "pdf": 14, "epub": 16,
                 "blogs": 10, "journals": 13 }

    pagetypes = { "accupages": 3, "conferencepages": 4 }

    try:
        db = pymysql.connect(args.host, 'accuorg_xarad', args.password, 'accuorg_xar', port=args.port, charset='latin1')
        if args.pubtype == 'bookreviews':
            dump_bookreviews(db, args.outputdir)
        elif args.pubtype.endswith('pages'):
            dump_pages(db, args.outputdir, args.pubtype, pagetypes[args.pubtype])
        else:
            dump_articles(db, args.outputdir, args.pubtype, pubtypes[args.pubtype])
    except Exception as err:
        print("Database access failed: {}".format(err), file=sys.stderr)
        sys.exit(1)
    sys.exit(0)

if __name__ == "__main__":
    main()

# Local Variables:
# mode: Python
# End:
